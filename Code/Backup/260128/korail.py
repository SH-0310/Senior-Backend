import time, requests, logging, re, json
from datetime import datetime, timedelta
from bs4 import BeautifulSoup

# âœ… ê³µí†µ ëª¨ë“ˆ ì„í¬íŠ¸
from utils import classify_categories, get_db_connection

# --- ì„¤ì • ë° ìƒìˆ˜ ---
AGENCY_NAME = "ì½”ë ˆì¼ê´€ê´‘ê°œë°œ"
KORAIL_PHONE = "1544-7755"
TELEGRAM_TOKEN = "8543857876:AAFs2kEURQEihK6_j6mw2PPaKQO4gYoBoSM"
CHAT_ID = "8305877092"

logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler("/home/ubuntu/Senior/Code/korail_crawler.log", encoding='utf-8'), 
        logging.StreamHandler()
    ]
)

def send_telegram_msg(text):
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    payload = {"chat_id": CHAT_ID, "text": text}
    try: requests.post(url, data=payload, timeout=10)
    except: pass

def get_detailed_data(parent_url, headers):
    """
    [ê°œì„ ] ìƒì„¸ í˜ì´ì§€ì˜ JSë¥¼ ë¶„ì„í•˜ì—¬ {ë‚ ì§œ: ê³ ìœ ë²ˆí˜¸} ë§¤í•‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    results = {"price": "0", "schedules": []}
    try:
        res = requests.get(parent_url, headers=headers, timeout=10)
        res.encoding = 'euc-kr'
        html = res.text
        soup = BeautifulSoup(html, 'html.parser')

        # 1. ì„±ì¸ ê°€ê²© ì¶”ì¶œ
        price_tag = soup.select_one('#adult_amt')
        if price_tag:
            results["price"] = price_tag.get_text(strip=True).replace(',', '')

        # 2. JS ë‚ ì§œ ë° ê³ ìœ ë²ˆí˜¸(selNum) ì¶”ì¶œ
        # ì˜ˆ: {"2026-01-28":"353130", ...}
        match = re.search(r'const\s+select_info\s*=\s*(\{.*?\});', html, re.DOTALL)
        if match:
            try:
                date_map = json.loads(match.group(1))
                for raw_date, sel_num in date_map.items():
                    clean_date = raw_date.replace('-', '') # DB ì €ì¥ìš© (20260128)
                    results["schedules"].append({
                        "clean_date": clean_date,
                        "raw_date": raw_date,      # URL íŒŒë¼ë¯¸í„°ìš© (2026-01-28)
                        "sel_num": sel_num         # URL íŒŒë¼ë¯¸í„°ìš© (353130)
                    })
            except: pass
        
        # 3. JS ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ì˜ ë°©ì–´ ë¡œì§ (ë°°ë„ˆ ë‚ ì§œ)
        if not results["schedules"]:
            rep_date_tag = soup.select_one('.Banner_txt strong')
            if rep_date_tag:
                nums = re.findall(r'\d+', rep_date_tag.get_text())
                if len(nums) >= 3:
                    date_val = f"{nums[0]}-{nums[1].zfill(2)}-{nums[2].zfill(2)}"
                    results["schedules"].append({
                        "clean_date": date_val.replace('-', ''),
                        "raw_date": date_val,
                        "sel_num": "" # ë²ˆí˜¸ë¥¼ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ë¹„ì›€
                    })
        return results
    except Exception as e:
        logging.error(f" ìƒì„¸ í˜ì´ì§€ ë¶„ì„ ì—ëŸ¬: {e}")
        return results

def run_collection():
    start_time = datetime.now()
    today_str = start_time.strftime("%Y%m%d")
    limit_str = (start_time + timedelta(days=30)).strftime("%Y%m%d")
    
    logging.info(f"ğŸš€ {AGENCY_NAME} '1ì¼ì†Œí’' ê³ ìœ  ë§í¬ ìˆ˜ì§‘ ì‹œì‘ ({today_str} ~ {limit_str})")
    
    conn = get_db_connection()
    conn.autocommit(True)
    stats = {"total_rprs": 0, "saved_schedules": 0, "deleted_tours": 0}

    try:
        page = 1
        with conn.cursor() as cursor:
            while True:
                list_url = f"https://www.korailtravel.com/web/goods_view/index.asp?page_nm=goods_list&gotopage={page}&strEpart=11"
                res = requests.get(list_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
                res.encoding = 'euc-kr'
                soup = BeautifulSoup(res.text, 'html.parser')
                
                items = [item for item in soup.select('.tourBox') if item.select_one('#tourBox_Title_b')]
                if not items: break

                for item in items:
                    title_main = item.select_one('#tourBox_Title_b').get_text(strip=True)
                    title_sub = item.select_one('#tourBox_Title_s').get_text(strip=True)
                    
                    btn = item.select_one('.tourBox_Btn')
                    match = re.search(r'setList\(\d+,\s*(\d+)\)', btn.get('onclick', '')) if btn else None
                    if not match: continue
                    
                    goods_num = match.group(1)
                    # ìƒì„¸ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•œ ê¸°ë³¸ ë¶€ëª¨ URL
                    parent_url = f"https://www.korailtravel.com/web/goods_view/index.asp?page_nm=goods_day&goodsNum={goods_num}"

                    detail_data = get_detailed_data(parent_url, {'User-Agent': 'Mozilla/5.0'})
                    
                    # 1ê°œì›” ì´ë‚´ í•„í„°ë§ëœ ì¼ì •ë§Œ ì¶”ì¶œ
                    valid_items = [s for s in detail_data["schedules"] if today_str <= s["clean_date"] <= limit_str]
                    if not valid_items: continue

                    # 1. ë¶€ëª¨ ì €ì¥
                    categories = classify_categories(title_main)
                    loc_match = re.search(r'\[(.*?)\]', title_sub)
                    location = loc_match.group(1) if loc_match else "êµ­ë‚´"

                    cursor.execute("""
                        INSERT INTO tours (product_code, reference_code, title, description, location, collected_at, agency, category, phone, is_priority)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, 0)
                        ON DUPLICATE KEY UPDATE title=%s, description=%s, location=%s, category=%s, collected_at=%s
                    """, (goods_num, goods_num, title_main, title_sub, location, start_time, AGENCY_NAME, categories, KORAIL_PHONE,
                          title_main, title_sub, location, categories, start_time))
                    stats["total_rprs"] += 1

                    # 2. ìì‹ ì €ì¥ (ì¶œë°œì¼ë³„ ê³ ìœ  URL ìƒì„±)
                    for s in valid_items:
                        # ğŸ¯ [í•µì‹¬] ì¶œë°œì¼ê³¼ ê³ ìœ ë²ˆí˜¸ë¥¼ ê²°í•©í•œ ì „ìš© ì˜ˆì•½ ë§í¬ ìƒì„±
                        child_booking_url = f"https://www.korailtravel.com/web/goods_view/index.asp?page_nm=goods_day&selDate={s['raw_date']}&selNum={s['sel_num']}"
                        
                        cursor.execute("""
                            INSERT INTO tour_schedules (product_code, title, departure_date, price_text, booking_url, updated_at, last_verified_at, error_msg)
                            VALUES (%s, %s, %s, %s, %s, %s, %s, NULL)
                            ON DUPLICATE KEY UPDATE title=%s, price_text=%s, booking_url=%s, updated_at=%s, last_verified_at=%s, error_msg=NULL
                        """, (goods_num, title_main, s['clean_date'], detail_data["price"], child_booking_url, start_time, start_time,
                              title_main, detail_data["price"], child_booking_url, start_time, start_time))
                        stats["saved_schedules"] += 1

                    logging.info(f"   âœ… {goods_num} ë™ê¸°í™”: {len(valid_items)}ê°œ ì¼ì • (ê³ ìœ ë§í¬ ìƒì„± ì™„ë£Œ)")
                    time.sleep(0.4)

                if len(items) < 10: break
                page += 1

            # 3. Cleanup: 1ì‹œê°„ ì´ì „ ë°ì´í„° ì‚­ì œ
            cleanup_limit_time = start_time - timedelta(hours=1)
            cursor.execute("DELETE FROM tours WHERE agency = %s AND collected_at < %s", (AGENCY_NAME, cleanup_limit_time))
            stats["deleted_tours"] = cursor.rowcount

            # âœ… [ì¶”ê°€] 3-1. í¬ë¡¤ë§ ì„±ê³µ ë¡œê·¸ ê¸°ë¡ (DB ì €ì¥)
            finish_time = datetime.now()
            log_sql = """
                INSERT INTO crawler_logs (agency_name, status, collected_count, crawled_at, message)
                VALUES (%s, %s, %s, %s, %s)
            """
            log_message = f"ë¶€ëª¨ {stats['total_rprs']}ì¢…, ì‚­ì œ {stats['deleted_tours']}ì¢… ìˆ˜ì§‘ ì™„ë£Œ"
            cursor.execute(log_sql, (
                AGENCY_NAME, 
                "SUCCESS", 
                stats["saved_schedules"], 
                finish_time, 
                log_message
            ))

        duration = datetime.now() - start_time
        report = f"ğŸ¤– [{AGENCY_NAME} '1ì¼ì†Œí’' ë™ê¸°í™” ì™„ë£Œ]\nâ± ì†Œìš”ì‹œê°„: {str(duration).split('.')[0]}\nğŸ“¦ ìœ íš¨ ìƒí’ˆ: {stats['total_rprs']}ì¢…\nğŸ”¹ 1ê°œì›” ë‚´ ì¼ì •: {stats['saved_schedules']}ê±´\nğŸ§¹ ì‚­ì œ: {stats['deleted_tours']}ì¢…"
        send_telegram_msg(report)

    except Exception as e:
        logging.error(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        # âœ… [ì¶”ê°€] ì—ëŸ¬ ë°œìƒ ì‹œ FAIL ìƒíƒœë¡œ ë¡œê·¸ ë‚¨ê¸°ê¸°
        try:
            with get_db_connection() as err_conn:
                with err_conn.cursor() as err_cursor:
                    err_cursor.execute("""
                        INSERT INTO crawler_logs (agency_name, status, crawled_at, message)
                        VALUES (%s, %s, %s, %s)
                    """, (AGENCY_NAME, "FAIL", datetime.now(), str(e)[:200]))
                    err_conn.commit()
        except: pass
        
        send_telegram_msg(f"âŒ {AGENCY_NAME} ì¹˜ëª…ì  ì˜¤ë¥˜: {str(e)[:100]}")
    finally:
        if 'conn' in locals() and conn: conn.close()

if __name__ == "__main__":
    run_collection()