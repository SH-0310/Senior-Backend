import time, requests, logging, re, json
from datetime import datetime, timedelta
from bs4 import BeautifulSoup

# âœ… ê³µí†µ ëª¨ë“ˆ ì„í¬íŠ¸
from utils import extract_all_keywords, get_db_connection

# --- ì„¤ì • ë° ìƒìˆ˜ ---
AGENCY_NAME = "ì½”ë ˆì¼ê´€ê´‘ê°œë°œ"
KORAIL_PHONE = "1544-7755"
TELEGRAM_TOKEN = "8543857876:AAFs2kEURQEihK6_j6mw2PPaKQO4gYoBoSM"
CHAT_ID = "8305877092"

logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler("/home/ubuntu/Senior/Code/korail_crawler.log", encoding='utf-8'), 
        logging.StreamHandler()
    ]
)

def send_telegram_msg(text):
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    payload = {"chat_id": CHAT_ID, "text": text}
    try: requests.post(url, data=payload, timeout=10)
    except: pass

def get_detailed_data(parent_url, headers):
    """
    ìƒì„¸ í˜ì´ì§€ì˜ JSë¥¼ ë¶„ì„í•˜ì—¬ {ë‚ ì§œ: ê³ ìœ ë²ˆí˜¸} ë§¤í•‘ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    results = {"price": "0", "schedules": []}
    try:
        res = requests.get(parent_url, headers=headers, timeout=10)
        res.encoding = 'euc-kr'
        html = res.text
        soup = BeautifulSoup(html, 'html.parser')

        # 1. ì„±ì¸ ê°€ê²© ì¶”ì¶œ
        price_tag = soup.select_one('#adult_amt')
        if price_tag:
            results["price"] = price_tag.get_text(strip=True).replace(',', '')

        # 2. JS ë‚ ì§œ ë° ê³ ìœ ë²ˆí˜¸(selNum) ì¶”ì¶œ
        # ì˜ˆ: {"2026-01-28":"353130", ...}
        match = re.search(r'const\s+select_info\s*=\s*(\{.*?\});', html, re.DOTALL)
        if match:
            try:
                date_map = json.loads(match.group(1))
                for raw_date, sel_num in date_map.items():
                    # âœ… [ìˆ˜ì •] DB ì €ì¥ìš© ë‚ ì§œ í˜•ì‹ì„ í•˜ì´í”ˆ í¬í•¨(YYYY-MM-DD)ìœ¼ë¡œ ìœ ì§€
                    results["schedules"].append({
                        "db_date": raw_date,       # DB ì €ì¥ìš© (2026-01-28)
                        "sel_num": sel_num         # URL íŒŒë¼ë¯¸í„°ìš©
                    })
            except: pass
        
        # 3. JS ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš°ì˜ ë°©ì–´ ë¡œì§
        if not results["schedules"]:
            rep_date_tag = soup.select_one('.Banner_txt strong')
            if rep_date_tag:
                nums = re.findall(r'\d+', rep_date_tag.get_text())
                if len(nums) >= 3:
                    date_val = f"{nums[0]}-{nums[1].zfill(2)}-{nums[2].zfill(2)}"
                    results["schedules"].append({
                        "db_date": date_val,
                        "sel_num": "" 
                    })
        return results
    except Exception as e:
        logging.error(f" ìƒì„¸ í˜ì´ì§€ ë¶„ì„ ì—ëŸ¬: {e}")
        return results

def run_collection():
    start_time = datetime.now()
    # âœ… [ìˆ˜ì •] ë¹„êµìš© ë‚ ì§œ ì—­ì‹œ í•˜ì´í”ˆ í¬í•¨ í˜•ì‹ìœ¼ë¡œ ì„¤ì •
    today_str = start_time.strftime("%Y-%m-%d")
    limit_str = (start_time + timedelta(days=30)).strftime("%Y-%m-%d")
    
    logging.info(f"ğŸš€ {AGENCY_NAME} ìˆ˜ì§‘ ì‹œì‘ ({today_str} ~ {limit_str})")
    
    conn = get_db_connection()
    conn.autocommit(True)
    stats = {"total_rprs": 0, "saved_schedules": 0, "deleted_tours": 0}

    try:
        page = 1
        with conn.cursor() as cursor:
            while True:
                list_url = f"https://www.korailtravel.com/web/goods_view/index.asp?page_nm=goods_list&gotopage={page}&strEpart=11"
                res = requests.get(list_url, headers={'User-Agent': 'Mozilla/5.0'}, timeout=10)
                res.encoding = 'euc-kr'
                soup = BeautifulSoup(res.text, 'html.parser')
                
                items = [item for item in soup.select('.tourBox') if item.select_one('#tourBox_Title_b')]
                if not items: break

                for item in items:
                    title_main = item.select_one('#tourBox_Title_b').get_text(strip=True)
                    title_sub = item.select_one('#tourBox_Title_s').get_text(strip=True)
                    
                    btn = item.select_one('.tourBox_Btn')
                    match = re.search(r'setList\(\d+,\s*(\d+)\)', btn.get('onclick', '')) if btn else None
                    if not match: continue
                    
                    goods_num = match.group(1)
                    parent_url = f"https://www.korailtravel.com/web/goods_view/index.asp?page_nm=goods_day&goodsNum={goods_num}"

                    detail_data = get_detailed_data(parent_url, {'User-Agent': 'Mozilla/5.0'})
                    
                    # ê¸°ê°„ ë‚´ ìœ íš¨ ìƒí’ˆ í•„í„°ë§
                    valid_items = [s for s in detail_data["schedules"] if today_str <= s["db_date"] <= limit_str]
                    if not valid_items: continue

                    # âœ… [ì¶”ê°€] íƒœê·¸ ì¶”ì¶œ
                    tags = extract_all_keywords(title_main)

                    # 1. ë¶€ëª¨ ì €ì¥
                    loc_match = re.search(r'\[(.*?)\]', title_sub)
                    location = loc_match.group(1) if loc_match else "êµ­ë‚´"

                    cursor.execute("""
                        INSERT INTO tours (product_code, reference_code, title, description, location, collected_at, agency, category, phone, is_priority)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, 0)
                        ON DUPLICATE KEY UPDATE title=%s, description=%s, location=%s, category=%s, collected_at=%s
                    """, (goods_num, goods_num, title_main, title_sub, location, start_time, AGENCY_NAME, tags, KORAIL_PHONE,
                          title_main, title_sub, location, tags, start_time))
                    stats["total_rprs"] += 1

                    # 2. ìì‹ ì €ì¥
                    for s in valid_items:
                        child_booking_url = f"https://www.korailtravel.com/web/goods_view/index.asp?page_nm=goods_day&selDate={s['db_date']}&selNum={s['sel_num']}"
                        
                        cursor.execute("""
                            INSERT INTO tour_schedules (product_code, title, departure_date, price_text, booking_url, updated_at, last_verified_at, tags)
                            VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                            ON DUPLICATE KEY UPDATE 
                                title=%s, price_text=%s, booking_url=%s, updated_at=%s, last_verified_at=%s, tags=%s,
                                departure_date=%s
                        """, (goods_num, title_main, s['db_date'], detail_data["price"], child_booking_url, start_time, start_time, tags,
                              title_main, detail_data["price"], child_booking_url, start_time, start_time, tags, s['db_date']))
                        stats["saved_schedules"] += 1

                    logging.info(f"   âœ… {goods_num} ë™ê¸°í™” ì™„ë£Œ ({len(valid_items)}ê°œ ì¼ì •)")
                    time.sleep(0.4)

                if len(items) < 10: break
                page += 1

            # 3. Cleanup & Logging
            cleanup_limit_time = start_time - timedelta(hours=1)
            cursor.execute("DELETE FROM tours WHERE agency = %s AND collected_at < %s", (AGENCY_NAME, cleanup_limit_time))
            stats["deleted_tours"] = cursor.rowcount

            finish_time = datetime.now()
            cursor.execute("""
                INSERT INTO crawler_logs (agency_name, status, collected_count, crawled_at, message)
                VALUES (%s, %s, %s, %s, %s)
            """, (AGENCY_NAME, "SUCCESS", stats["saved_schedules"], finish_time, f"ë¶€ëª¨ {stats['total_rprs']}ì¢… ìˆ˜ì§‘"))

        duration = datetime.now() - start_time
        send_telegram_msg(f"ğŸ¤– [{AGENCY_NAME} ì™„ë£Œ]\nğŸ“¦ ìœ íš¨ ìƒí’ˆ: {stats['total_rprs']}ì¢…\nğŸ”¹ ìì‹ ì¼ì •: {stats['saved_schedules']}ê±´\nâ± ì†Œìš”ì‹œê°„: {str(duration).split('.')[0]}")

    except Exception as e:
        logging.error(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        send_telegram_msg(f"âŒ {AGENCY_NAME} ì˜¤ë¥˜: {str(e)[:100]}")
    finally:
        if 'conn' in locals() and conn: conn.close()

if __name__ == "__main__":
    run_collection()