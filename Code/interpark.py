import time, requests, logging, json
from datetime import datetime, timedelta
from utils import extract_all_keywords, get_db_connection

# --- ì„¤ì • ë° ìƒìˆ˜ ---
AGENCY_NAME = "ì¸í„°íŒŒí¬íˆ¬ì–´"
IP_PHONE = "1588-3443"
TELEGRAM_TOKEN = "8543857876:AAFs2kEURQEihK6_j6mw2PPaKQO4gYoBoSM"
CHAT_ID = "8305877092"

logging.basicConfig(
    level=logging.INFO, 
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler("/home/ubuntu/Senior/Code/interpark_crawler.log", encoding='utf-8'), 
        logging.StreamHandler()
    ]
)

def send_telegram_msg(text):
    url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    payload = {"chat_id": CHAT_ID, "text": text}
    try: requests.post(url, data=payload, timeout=10)
    except: pass

def run_collection():
    start_time = datetime.now()
    today_str = start_time.strftime("%Y%m%d")
    limit_str = (start_time + timedelta(days=30)).strftime("%Y%m%d")
    
    logging.info(f"ğŸš€ {AGENCY_NAME} ìˆ˜ì§‘ ì‹œì‘ (ê²€ìƒ‰ì–´: 'ë‹¹ì¼ì—¬í–‰', ë²”ìœ„: {today_str} ~ {limit_str})")
    
    api_url = "https://travel.interpark.com/api-package/search"
    headers = {
        'Content-Type': 'application/json',
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'application/json, text/plain, */*',
        'Origin': 'https://travel.interpark.com',
        'Referer': 'https://travel.interpark.com/tour/search'
    }

    stats = {"total_rprs": 0, "saved_schedules": 0, "deleted_tours": 0, "filtered_by_day": 0}
    conn = get_db_connection()
    conn.autocommit(True)

    try:
        with conn.cursor() as cursor:
            # 1. API í˜¸ì¶œ ë¡œê·¸ ê°•í™”
            payload = {
                "q": "ë‹¹ì¼ì—¬í–‰", "domain": "t", "resveCours": "p",
                "start": 0, "rows": 100, "sort": "score desc", "filter": []
            }
            
            logging.info(f"ğŸ“¡ API ìš”ì²­ ì¤‘... URL: {api_url}")
            res = requests.post(api_url, headers=headers, json=payload, timeout=15)
            
            if res.status_code != 200:
                logging.error(f"âŒ API í˜¸ì¶œ ì‹¤íŒ¨! ìƒíƒœ ì½”ë“œ: {res.status_code}")
                logging.error(f"ğŸ“„ ì‘ë‹µ ë‚´ìš©: {res.text[:500]}")
                return

            data = res.json()
            docs = data.get("data", {}).get("docs", [])
            total_count = data.get("data", {}).get("totalCount", 0)
            
            logging.info(f"âœ… API ì‘ë‹µ ìˆ˜ì‹ : ê²€ìƒ‰ê²°ê³¼ ì´ {total_count}ê°œ ì¤‘ {len(docs)}ê°œ ìˆ˜ì§‘ë¨")

            if not docs:
                logging.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼(docs)ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ê²€ìƒ‰ì–´ 'ë‹¹ì¼ì—¬í–‰'ì„ 'ë‹¹ì¼'ë¡œ ë³€ê²½í•˜ê±°ë‚˜ í˜ì´ë¡œë“œë¥¼ í™•ì¸í•˜ì„¸ìš”.")

            for idx, p_doc in enumerate(docs, 1):
                parent_title = p_doc.get("goodsNm", "ì œëª©ì—†ìŒ")
                parent_code = p_doc.get("baseGoodsCode") or p_doc.get("goodsCode")
                tour_day = p_doc.get("tourDay") or ""
                
                # ë¡œê·¸: ëª¨ë“  ìƒí’ˆì˜ ì œëª©ê³¼ ì—¬í–‰ ì¼ìˆ˜ ë…¸ì¶œ
                logging.info(f"ğŸ” [{idx}/{len(docs)}] ìƒí’ˆ ë¶„ì„: {parent_title} (ì½”ë“œ: {parent_code}, ê¸°ê°„: {tour_day})")

                # í•„í„°ë§ ë¡œê·¸
                if "0ë°•1ì¼" not in tour_day:
                    logging.debug(f"   â© íŒ¨ìŠ¤: '0ë°•1ì¼' ì•„ë‹˜ ({tour_day})")
                    stats["filtered_by_day"] += 1
                    continue

                main_img_url = p_doc.get("mainImgUrl", "")
                region_list = p_doc.get("stdRegionNm") or []
                location = region_list[0] if region_list else "êµ­ë‚´"
                categories = extract_all_keywords(parent_title)
                description = p_doc.get("productDescription") or parent_title

                # 1) ë¶€ëª¨ ì €ì¥
                cursor.execute("""
                    INSERT INTO tours (product_code, reference_code, title, description, main_image_url, location, collected_at, agency, category, phone)
                    VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON DUPLICATE KEY UPDATE title=%s, main_image_url=%s, collected_at=%s
                """, (parent_code, parent_code, parent_title, description, main_img_url, location, start_time, AGENCY_NAME, categories, IP_PHONE,
                    parent_title, main_img_url, start_time))
                stats["total_rprs"] += 1
                
                # 2) ì¼ì • ë³‘í•© ë¡œê·¸
                sub_docs_container = p_doc.get("subDocs") or {}
                sub_list = sub_docs_container.get("docs") or []
                all_raw_docs = [p_doc] + sub_list
                
                logging.info(f"   ğŸ“¦ ë¶€ëª¨ ìƒí’ˆ ì €ì¥ ì™„ë£Œ. ì—°ê²°ëœ ì¼ì • í›„ë³´: {len(all_raw_docs)}ê°œ")

                seen_dates = set()
                valid_count_for_this_tour = 0
                
                for c_idx, c_doc in enumerate(all_raw_docs):
                    dep_date_raw = c_doc.get("departureDay")
                    
                    if not dep_date_raw:
                        continue
                    
                    # ë‚ ì§œ í•„í„°ë§ ìƒì„¸ ë¡œê·¸
                    if not (today_str <= dep_date_raw <= limit_str):
                        logging.debug(f"      ğŸ“… ë‚ ì§œ ì œì™¸: {dep_date_raw} (ë²”ìœ„ ë°–)")
                        continue
                        
                    if dep_date_raw in seen_dates:
                        continue

                    seen_dates.add(dep_date_raw)
                    dep_date_db = f"{dep_date_raw[:4]}-{dep_date_raw[4:6]}-{dep_date_raw[6:]}"
                    child_code = c_doc.get("goodsCode") or parent_code
                    price = c_doc.get("salesPrice") or c_doc.get("price") or 0
                    tags = extract_all_keywords(parent_title)
                    
                    raw_status = c_doc.get("bookingCode")
                    status = raw_status if raw_status not in ["ì˜ˆì•½ê°€ëŠ¥", "ì¶œë°œí™•ì •", None] else None
                    booking_url = f"https://travel.interpark.com/tour/goods?goodsCd={child_code}"

                    cursor.execute("""
                        INSERT INTO tour_schedules (product_code, title, departure_date, price_text, booking_url, updated_at, last_verified_at, error_msg, tags)
                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                        ON DUPLICATE KEY UPDATE price_text=%s, updated_at=%s, last_verified_at=%s, error_msg=%s, tags=%s, departure_date=%s
                    """, (parent_code, parent_title, dep_date_db, str(price), booking_url, start_time, start_time, status, tags,
                          str(price), start_time, start_time, status, tags, dep_date_db))
                    
                    valid_count_for_this_tour += 1
                    logging.info(f"      âˆŸ ğŸ“… {dep_date_db} | ğŸ’° {price}ì› | ğŸ· {status or 'ì •ìƒ'}")

                stats["saved_schedules"] += valid_count_for_this_tour

            # ğŸ›  [Cleanup]
            cleanup_limit_time = start_time - timedelta(hours=1)
            cursor.execute("DELETE FROM tours WHERE agency = %s AND collected_at < %s", (AGENCY_NAME, cleanup_limit_time))
            stats["deleted_tours"] = cursor.rowcount

            # ğŸ“Š ë¡œê·¸ ê¸°ë¡
            finish_time = datetime.now()
            log_sql = "INSERT INTO crawler_logs (agency_name, status, collected_count, crawled_at, message) VALUES (%s, %s, %s, %s, %s)"
            log_message = f"ë¶€ëª¨ {stats['total_rprs']}ì¢…(ë¹„ë‹¹ì¼íŒ¨ìŠ¤ {stats['filtered_by_day']}ì¢…), ì‚­ì œ {stats['deleted_tours']}ì¢…"
            cursor.execute(log_sql, (AGENCY_NAME, "SUCCESS", stats["saved_schedules"], finish_time, log_message))

        duration = datetime.now() - start_time
        report = (
            f"ğŸ¤– [{AGENCY_NAME} ìˆ˜ì§‘ ë¦¬í¬íŠ¸]\n"
            f"ê²€ìƒ‰ê²°ê³¼: {total_count}ê°œ\n"
            f"ë‹¹ì¼ë¶€ëª¨: {stats['total_rprs']}ì¢…\n"
            f"ì¼ì •ì €ì¥: {stats['saved_schedules']}ê±´\n"
            f"ë¹„ë‹¹ì¼ì œì™¸: {stats['filtered_by_day']}ê±´"
        )
        send_telegram_msg(report)
        logging.info(f"ğŸ ìˆ˜ì§‘ ì¢…ë£Œ. ì €ì¥ëœ ì¼ì •: {stats['saved_schedules']}ê±´. ì†Œìš”ì‹œê°„: {duration}")

    except Exception as e:
        logging.error(f"ğŸ’¥ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}", exc_info=True)
        try:
            with get_db_connection() as err_conn:
                with err_conn.cursor() as err_cursor:
                    err_cursor.execute("INSERT INTO crawler_logs (agency_name, status, crawled_at, message) VALUES (%s, %s, %s, %s)", 
                                     (AGENCY_NAME, "FAIL", datetime.now(), str(e)[:200]))
                    err_conn.commit()
        except: pass
        send_telegram_msg(f"âŒ {AGENCY_NAME} ìˆ˜ì§‘ ì‹¤íŒ¨: {str(e)[:100]}")
    finally:
        if 'conn' in locals() and conn: conn.close()

if __name__ == "__main__":
    run_collection()